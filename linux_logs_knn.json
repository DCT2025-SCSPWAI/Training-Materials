{"id":"61d26639-a746-4e3a-905f-e3941e275e40","createdAt":1753497961875,"updatedAt":1753589243873,"text":"## SETUP\n\nPUT /_cluster/settings\n{\n    \"persistent\": {\n        \"plugins.ml_commons.trusted_connector_endpoints_regex\": [\n          \"^https://runtime\\\\.sagemaker\\\\..*[a-z0-9-]\\\\.amazonaws\\\\.com/.*$\",\n          \"^https://api\\\\.openai\\\\.com/.*$\",\n          \"^https://api\\\\.cohere\\\\.ai/.*$\",\n          \"^https://bedrock-runtime\\\\..*[a-z0-9-]\\\\.amazonaws\\\\.com/.*$\",\n          \"^https://.*$\",\n          \"^http://.*$\"\n        ]\n    }\n}\n\nPUT /_cluster/settings \n{ \n  \"persistent\": { \n    \"plugins.ml_commons.memory_feature_enabled\": false, \n    \"plugins.ml_commons.rag_pipeline_feature_enabled\": true, \n    \"plugins.ml_commons.agent_framework_enabled\": true, \n    \"plugins.ml_commons.only_run_on_ml_node\": false, \n    \"plugins.ml_commons.connector_access_control_enabled\": true, \n    \"plugins.ml_commons.model_access_control_enabled\": false,\n    \"plugins.ml_commons.native_memory_threshold\": 99,\n    \"plugins.ml_commons.allow_registering_model_via_local_file\": true,\n    \"plugins.ml_commons.allow_registering_model_via_url\": true,\n    \"plugins.ml_commons.model_auto_redeploy.enable\":true,\n    \"plugins.ml_commons.model_auto_redeploy.lifetime_retry_times\": 10\n  } \n} \n\n\nGET _cat/plugins\n\n#Imported on Jul 19, 2025 @ 21:14:29.893\n\n############################################################\n############################################################\n############################################################\n\n\n## Local Model For Embeddings\n## This is the Embedding model used to add vectors to our data. \n## We are utilizing a 768-vector sparse space searching model\nPOST /_plugins/_ml/models/_register\n{\n  \"name\": \"huggingface/sentence-transformers/all-mpnet-base-v2\",\n  \"version\": \"1.0.2\",\n  \"model_format\": \"TORCH_SCRIPT\"\n}\n\n##Utilize the Task ID from the previous output\nGET /_plugins/_ml/tasks/DRKWKZgB3rgc35q2FrG0\n\n## Utilize the ModelID from the previous output\n## There is a delay for this command to actually complete. Wait for the state to swap to COMPLETED, and the model_id should appear at the top. You will have to re-run the above command a few times to grab the information\nPOST /_plugins/_ml/models/EBKWKZgB3rgc35q2GLG9/_deploy\n\n## Use the ModelID from the previous output\n## This will let you monitor the status of the deployment of the embedding model\nGET /_plugins/_ml/models/EBKWKZgB3rgc35q2GLG9\n\n## This tests the embedding model to ensure it is running\n## You should see a string of numbers appear on the right side\nPOST /_plugins/_ml/models/EBKWKZgB3rgc35q2GLG9/_predict\n{\n  \"text_docs\":[ \"What is Stuxnet?\"],\n  \"return_number\": true,\n  \"target_response\": [\"sentence_embedding\"]\n}\n\n## Making Pipeline\n## This creates the pipeline to allow the embedding model to apply vector values to our data\n## The ModelID here is that of the embedding Model\n## Make sure you keep the ModelID for the Embedding Model and Interference Model noted down\nPUT /_ingest/pipeline/linux-logs-nlp-ingest-pipeline\n{\n    \"description\": \"text embedding pipeline\",\n    \"processors\": [\n        {\n            \"text_embedding\": {\n                \"model_id\": \"EBKWKZgB3rgc35q2GLG9\",\n                \"field_map\": {\n                    \"log\": \"log_embedding\"\n                }\n            }\n        }\n    ]\n}\n\n## Making KNN Index\n## This creates an Index that allows embeddings/vector values to be assigned to our data\nPUT linux_logs_knn_test\n{\n  \"settings\": {\n    \"index.knn\": true,\n    \"default_pipeline\": \"linux-logs-nlp-ingest-pipeline\"\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"id\": {\n        \"type\": \"text\"\n      },\n      \"log_embedding\": {\n        \"type\": \"knn_vector\",\n        \"dimension\": 768,\n        \"method\": {\n          \"engine\": \"lucene\",\n          \"space_type\": \"cosinesimil\",\n          \"name\": \"hnsw\",\n          \"parameters\": {}\n        }\n      },\n      \"text\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n\n\n## Adding Data to Index\n## Data added via fluent-bit\n\nGET linux_logs_knn_test\n\n##Validating data made it in\n## This may not provide output if data was ingested via fluentbit\nGET linux_logs_knn_test/_doc/1\n\n\n## Making the Connector\n## This created the connector for the semantic model\nPOST /_plugins/_ml/connectors/_create\n{\n  \"name\": \"LMStudio Connector - linux_logs_knn - Development\",\n  \"description\": \"Development LMStudio Host\",\n  \"version\": \"2\",\n  \"protocol\": \"http\",\n  \"parameters\": {\n    \"endpoint\": \"http://20.106.179.227:5000\",\n    \"model\": \"openvoid/prox-7b-dpo-gguf\",\n    \"max tokens\": 2000,\n    \"temperature\": 0.5\n  },\n  \"credential\": {\n    \"openAI_key\": \"2025unchiphopsummer\"\n    },\n  \"actions\": [\n    {\n      \"action_type\": \"predict\",\n      \"method\": \"POST\",\n      \"url\": \"${parameters.endpoint}/v1/chat/completions\",\n      \"request_body\": \"\"\"{ \"model\": \"${parameters.model}\", \"messages\": ${parameters.messages}, \"temperature\": ${parameters.temperature} }\"\"\"\n    }\n  ]\n}\n\n## saving Connector ID: gnUmRZgBKJWqR4KGw4n7\n## This reads back the above input\nGET /_plugins/_ml/connectors/R3YMSpgBKJWqR4KGeC63\n\n\n## This Registers the interference model with Opensearch\n## This utilizes the connector ID from above\n## Make sure to note the Model ID output from this command\nPOST /_plugins/_ml/models/_register\n{\n  \"name\": \"linux-logs-test-knn-rag-Development\",\n  \"function_name\": \"remote\",\n  \"description\": \"openvoid Model on Development LMStudio Host\",\n  \"connector_id\": \"R3YMSpgBKJWqR4KGeC63\"\n}\n\n## This deploys/enables the connector for the semantic model\n## Use the model ID from above\nPOST /_plugins/_ml/models/TXYMSpgBKJWqR4KGpS5d/_deploy\n\n## This sends a simple query to the LLM on LMStudio\n## There is no RAG associated with this command. It is the same as simply asking LMStudio from the local interface\nPOST /_plugins/_ml/models/TXYMSpgBKJWqR4KGpS5d/_predict\n{\n  \"parameters\": {\n    \"messages\": [\n      {\n        \"role\": \"assistant\",\n        \"content\": \"What logs are here?\"\n      }\n    ],\n    \"temperature\": 0.5\n  }\n}\n\n\n## Register RAGTool with flow agent\n## This flow agent combines the embedding model with the interference model\n## Since we have a linear flow of commands, the embedding model is registered and used first\n## The index that the embedding model searches is re-identified here\n## Next is the actual interference model\n## You can also see the system prompt that is being passed to LMStudio\nPOST /_plugins/_ml/agents/_register\n{\n  \"name\": \"Test_Agent_For_RAG-linux-logs\",\n  \"type\": \"flow\",\n  \"description\": \"This is a test agent using the Development LMStudio host\",\n  \"tools\": [\n    {\n      \"type\": \"VectorDBTool\",\n      \"parameters\": {\n        \"model_id\": \"EBKWKZgB3rgc35q2GLG9\",\n        \"index\": \"linux_logs_knn_test\",\n        \"embedding_field\": \"log_embedding\",\n        \"source_field\": [\"log\"],\n        \"input\": \"${parameters.question}\"\n      }\n    },\n    {\n      \"type\": \"MLModelTool\",\n      \"description\": \"A general tool to answer any question\",\n      \"parameters\": {\n        \"model_id\": \"TXYMSpgBKJWqR4KGpS5d\",\n        \"response_filter\": \"$.choices[0].message.content\",\n        \"messages\": [\n          {\n            \"role\": \"assistant\",\n            \"content\": \"\\n### Instruction:\\n You are a professional data analyst. You will always answer questions based on the given context first. If the answer is not directly shown in the context, you will analyze the data and find the answer. If you don't know the answer, just say 'don't know'.  \\n\\n Context:\\n${parameters.VectorDBTool.output}\\n\\nHuman:${parameters.question}\\n### Response:\\n\"\n          }\n        ],\n        \"temperature\": 0.5\n      }\n    }\n  ]\n}\n\n## This executes the embedding model against the specified index, and passes what is believed to be relevant content to the LLM running on LMStudio\nPOST /_plugins/_ml/agents/WHYOSpgBKJWqR4KGUy6-/_execute\n{\n  \"parameters\": {\n    \"question\": \"Are there anacron logs here?\"\n  }\n}\n\n############################################################\n############################################################\n############################################################"}